{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c71c75ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SyncPage[Model](data=[Model(id='gemma-3-27b-it', created=None, object='model', owned_by='organization_owner'), Model(id='gemma-3-4b-it-qat', created=None, object='model', owned_by='organization_owner'), Model(id='llava-v1.5-7b', created=None, object='model', owned_by='organization_owner'), Model(id='text-embedding-nomic-embed-text-v1.5@q4_k_m', created=None, object='model', owned_by='organization_owner'), Model(id='granite-vision-3.2-2b', created=None, object='model', owned_by='organization_owner'), Model(id='gemma-2-9b-it', created=None, object='model', owned_by='organization_owner'), Model(id='text-embedding-nomic-embed-text-v1.5@q8_0', created=None, object='model', owned_by='organization_owner')], object='list')\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "import pandas as pd\n",
    "import base64\n",
    "import io\n",
    "from PIL import Image\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Point to the local server\n",
    "client = OpenAI(base_url=\"http://localhost:1234/v1\", api_key=\"lm-studio\")\n",
    "print(client.models.list())\n",
    "model_name = 'gemma-3-27b-it'\n",
    "output_dir = f\"results_{model_name}\"\n",
    "character_schema = {\n",
    "    \"type\": \"json_schema\",\n",
    "    \"json_schema\": {\n",
    "        \"name\": \"Judgment\",\n",
    "        \"schema\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"Judgment\": {\"type\": \"string\"},\n",
    "                \"Reasons\": {\"type\": \"string\"}\n",
    "            },\n",
    "            \"required\": [\"Judgment\",\"Reasons\"]\n",
    "        },\n",
    "    }\n",
    "}\n",
    "\n",
    "# Create the folder if it doesn't exist\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1d3c86",
   "metadata": {},
   "source": [
    "### Zero-shot1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545ff351",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Accuracy (all): 59.00%\n",
      "✅ Accuracy (excluding 'equal'): 64.13%\n"
     ]
    }
   ],
   "source": [
    "# Load metadata\n",
    "prompt_method = \"Zero-shot1-GEO\"\n",
    "metadata_df = pd.read_csv(\"merged_metadata.csv\")\n",
    "image_dir = \"merged_images\"\n",
    "results = []\n",
    "\n",
    "for _, row in metadata_df.iterrows():\n",
    "    merged_index = row[\"merged_index\"]\n",
    "    study_question = row[\"study_question\"]\n",
    "    left_place = row[\"place_name_left\"]\n",
    "    right_place = row[\"place_name_right\"]\n",
    "    ground_truth = str(row[\"choice\"]).strip().lower()\n",
    "    image_path = os.path.join(image_dir, f\"merged_{merged_index:03d}.jpg\")\n",
    "\n",
    "    # Encode image to base64\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    img_byte_arr = io.BytesIO()\n",
    "    image.save(img_byte_arr, format=\"JPEG\")\n",
    "    base64_image = base64.b64encode(img_byte_arr.getvalue()).decode(\"utf-8\")\n",
    "\n",
    "    # Compose LLM prompt\n",
    "    prompt_text = f\"\"\"\n",
    "                    You are shown a side-by-side image with two street views at two different cities: the left half at {left_place} and the right half at {right_place}.\n",
    "                    Which side looks more {study_question}?\n",
    "\n",
    "                    Answer with only one word: \"left\", \"right\", or \"equal\". Then explain your reasoning.\n",
    "\n",
    "                    Format:\n",
    "                    Judgment. Reasons.\n",
    "                    \"\"\"\n",
    "\n",
    "    # Call LLM \n",
    "    response = client.chat.completions.create(\n",
    "        model= model_name,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": prompt_text.strip()},\n",
    "                    {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\"}},\n",
    "                ],\n",
    "            },\n",
    "        ],\n",
    "        response_format=character_schema,\n",
    "    )\n",
    "    full_response = json.loads(response.choices[0].message.content)\n",
    "\n",
    "    # Split judgment and reasoning\n",
    "    # Extract from dict\n",
    "    model_judgement = full_response.get(\"Judgment\", \"\").strip().lower()\n",
    "    model_reason = full_response.get(\"Reasons\", \"\").strip()\n",
    "\n",
    "    results.append({\n",
    "        \"merged_index\": merged_index,\n",
    "        \"left\": row[\"left\"],\n",
    "        \"right\": row[\"right\"],\n",
    "        \"study_question\": study_question,\n",
    "        \"ground_truth\": ground_truth,\n",
    "        \"model_judgement\": model_judgement,\n",
    "        \"model_reason\": model_reason,\n",
    "        \"validation\": int(model_judgement == ground_truth)\n",
    "    })\n",
    "\n",
    "# Save results\n",
    "df_result = pd.DataFrame(results)\n",
    "df_result.to_csv(f\"{output_dir}/llm_predictions_{model_name}_{prompt_method}.csv\", index=False)\n",
    "\n",
    "# Print accuracy\n",
    "# Accuracy including all responses\n",
    "accuracy_all = df_result[\"validation\"].mean()\n",
    "\n",
    "# Accuracy excluding any 'equal' in ground truth or model judgement\n",
    "filtered_df = df_result[\n",
    "    (df_result[\"ground_truth\"] != \"equal\") & \n",
    "    (df_result[\"model_judgement\"] != \"equal\")\n",
    "]\n",
    "accuracy_excl_equal = filtered_df[\"validation\"].mean()\n",
    "\n",
    "# Print both\n",
    "print(f\"✅ Accuracy (all): {accuracy_all:.2%}\")\n",
    "print(f\"✅ Accuracy (excluding 'equal'): {accuracy_excl_equal:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d1e8aa",
   "metadata": {},
   "source": [
    "### Zero-Shot2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cade4630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Accuracy (all): 58.00%\n",
      "✅ Accuracy (excluding 'equal'): 62.37%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import base64\n",
    "import io\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# Load metadata\n",
    "prompt_method= \"Zero_Shot2-GEO\"\n",
    "metadata_df = pd.read_csv(\"merged_metadata.csv\")\n",
    "image_dir = \"merged_images\"\n",
    "results = []\n",
    "\n",
    "for _, row in metadata_df.iterrows():\n",
    "    merged_index = row[\"merged_index\"]\n",
    "    study_question = row[\"study_question\"]\n",
    "    left_place = row[\"place_name_left\"]\n",
    "    right_place = row[\"place_name_right\"]\n",
    "    ground_truth = str(row[\"choice\"]).strip().lower()\n",
    "    image_path = os.path.join(image_dir, f\"merged_{merged_index:03d}.jpg\")\n",
    "\n",
    "    # Encode image to base64\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    img_byte_arr = io.BytesIO()\n",
    "    image.save(img_byte_arr, format=\"JPEG\")\n",
    "    base64_image = base64.b64encode(img_byte_arr.getvalue()).decode(\"utf-8\")\n",
    "\n",
    "    # Compose updated prompt\n",
    "    prompt_text = f\"\"\"\n",
    "        Compare two street view images taken in different cities — the left image is from {left_place}, and the right image is from {right_place}. Based on the overall impression, decide which image better reflects the following quality:\n",
    "\n",
    "        \"{study_question}\"\n",
    "\n",
    "        Respond with a one-word judgment: left, right, or equal. Then explain your reasoning.\n",
    "\n",
    "        Format:\n",
    "        Judgment. Reasons.\n",
    "        \"\"\"\n",
    "\n",
    "    # Call LLM\n",
    "    response = client.chat.completions.create(\n",
    "        model= model_name,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": prompt_text.strip()},\n",
    "                    {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\"}},\n",
    "                ],\n",
    "            },\n",
    "        ],\n",
    "        response_format=character_schema,\n",
    "    )\n",
    "    full_response = json.loads(response.choices[0].message.content)\n",
    "\n",
    "    # Split judgment and reasoning\n",
    "    # Extract from dict\n",
    "    model_judgement = full_response.get(\"Judgment\", \"\").strip().lower()\n",
    "    model_reason = full_response.get(\"Reasons\", \"\").strip()\n",
    "\n",
    "    results.append({\n",
    "        \"merged_index\": merged_index,\n",
    "        \"left\": row[\"left\"],\n",
    "        \"right\": row[\"right\"],\n",
    "        \"study_question\": study_question,\n",
    "        \"ground_truth\": ground_truth,\n",
    "        \"model_judgement\": model_judgement,\n",
    "        \"model_reason\": model_reason,\n",
    "        \"validation\": int(model_judgement == ground_truth)\n",
    "    })\n",
    "\n",
    "# Save results\n",
    "df_result = pd.DataFrame(results)\n",
    "df_result.to_csv(f\"{output_dir}/llm_predictions_{model_name}_{prompt_method}.csv\", index=False)\n",
    "\n",
    "# Print accuracy\n",
    "# Accuracy including all responses\n",
    "accuracy_all = df_result[\"validation\"].mean()\n",
    "\n",
    "# Accuracy excluding any 'equal' in ground truth or model judgement\n",
    "filtered_df = df_result[\n",
    "    (df_result[\"ground_truth\"] != \"equal\") & \n",
    "    (df_result[\"model_judgement\"] != \"equal\")\n",
    "]\n",
    "accuracy_excl_equal = filtered_df[\"validation\"].mean()\n",
    "\n",
    "# Print both\n",
    "print(f\"✅ Accuracy (all): {accuracy_all:.2%}\")\n",
    "print(f\"✅ Accuracy (excluding 'equal'): {accuracy_excl_equal:.2%}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8f7fb2",
   "metadata": {},
   "source": [
    "### Chain-of-Thought"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ecf9eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Accuracy (all): 60.00%\n",
      "✅ Accuracy (excluding 'equal'): 64.52%\n",
      "✅ Output saved to llm_predictions.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import base64\n",
    "import io\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# Load metadata\n",
    "prompt_method= \"COT-GEO\"\n",
    "metadata_df = pd.read_csv(\"merged_metadata.csv\")\n",
    "image_dir = \"merged_images\"\n",
    "results = []\n",
    "\n",
    "for _, row in metadata_df.iterrows():\n",
    "    merged_index = row[\"merged_index\"]\n",
    "    study_question = row[\"study_question\"]\n",
    "    left_place = row[\"place_name_left\"]\n",
    "    right_place = row[\"place_name_right\"]\n",
    "    ground_truth = str(row[\"choice\"]).strip().lower()\n",
    "    image_path = os.path.join(image_dir, f\"merged_{merged_index:03d}.jpg\")\n",
    "\n",
    "    # Encode image to base64\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    img_byte_arr = io.BytesIO()\n",
    "    image.save(img_byte_arr, format=\"JPEG\")\n",
    "    base64_image = base64.b64encode(img_byte_arr.getvalue()).decode(\"utf-8\")\n",
    "\n",
    "    # Compose updated prompt\n",
    "    prompt_text = f\"\"\"\n",
    "        Let’s think step by step.\n",
    "        Compare two street view images taken in different cities — the left image is from {left_place}, and the right image is from {right_place}. \n",
    "        Based on your overall impression, determine which one better reflects the following quality:\n",
    "\n",
    "        \"{study_question}\"\n",
    "\n",
    "        What features do you see in the left image? What features in the left image contribute to or detract from that quality?\n",
    "\n",
    "        What features do you see in the right image? What features in the right image contribute to or detract from that quality?\n",
    "\n",
    "        Based on your reasoning, which image better reflects the quality?\n",
    "\n",
    "        Respond with a one-word judgment: left, right, or equal. Then explain your reasoning.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "    # Call LLM\n",
    "    response = client.chat.completions.create(\n",
    "        model= model_name,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": prompt_text.strip()},\n",
    "                    {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\"}},\n",
    "                ],\n",
    "            },\n",
    "        ],\n",
    "        response_format=character_schema,\n",
    "    )\n",
    "    full_response = json.loads(response.choices[0].message.content)\n",
    "\n",
    "    # Split judgment and reasoning\n",
    "    # Extract from dict\n",
    "    model_judgement = full_response.get(\"Judgment\", \"\").strip().lower()\n",
    "    model_reason = full_response.get(\"Reasons\", \"\").strip()\n",
    "\n",
    "    results.append({\n",
    "        \"merged_index\": merged_index,\n",
    "        \"left\": row[\"left\"],\n",
    "        \"right\": row[\"right\"],\n",
    "        \"study_question\": study_question,\n",
    "        \"ground_truth\": ground_truth,\n",
    "        \"model_judgement\": model_judgement,\n",
    "        \"model_reason\": model_reason,\n",
    "        \"validation\": int(model_judgement == ground_truth)\n",
    "    })\n",
    "\n",
    "# Save results\n",
    "df_result = pd.DataFrame(results)\n",
    "df_result.to_csv(f\"{output_dir}/llm_predictions_{model_name}_{prompt_method}.csv\", index=False)\n",
    "\n",
    "# Print accuracy\n",
    "# Accuracy including all responses\n",
    "accuracy_all = df_result[\"validation\"].mean()\n",
    "\n",
    "# Accuracy excluding any 'equal' in ground truth or model judgement\n",
    "filtered_df = df_result[\n",
    "    (df_result[\"ground_truth\"] != \"equal\") & \n",
    "    (df_result[\"model_judgement\"] != \"equal\")\n",
    "]\n",
    "accuracy_excl_equal = filtered_df[\"validation\"].mean()\n",
    "\n",
    "# Print both\n",
    "print(f\"✅ Accuracy (all): {accuracy_all:.2%}\")\n",
    "print(f\"✅ Accuracy (excluding 'equal'): {accuracy_excl_equal:.2%}\")\n",
    "print(\"✅ Output saved to llm_predictions.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54751f9f",
   "metadata": {},
   "source": [
    "### Rule-Base + In-context learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f38cb4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Accuracy (all): 57.00%\n",
      "✅ Accuracy (excluding 'equal'): 66.28%\n",
      "✅ Output saved to llm_predictions.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import base64\n",
    "import io\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# Load metadata\n",
    "prompt_method= \"RBIL-GEO\"\n",
    "metadata_df = pd.read_csv(\"merged_metadata.csv\")\n",
    "image_dir = \"merged_images\"\n",
    "results = []\n",
    "\n",
    "def get_visual_perspective(study_question):\n",
    "    rules = {\n",
    "        \"wealthier\": (\n",
    "            \"Look for expensive cars, clean sidewalks, modern buildings, well-maintained facades, greenery, and overall tidiness. \"\n",
    "            \"Signs of poverty such as trash, broken sidewalks, and older buildings detract from the feeling of wealth.\"\n",
    "        ),\n",
    "        \"more beautiful\": (\n",
    "            \"Aesthetics matter: symmetry, architectural design, vibrant colors, trees, flowers, sunlight, and open space. \"\n",
    "            \"Avoid cluttered, gray, damaged, or visually unappealing features.\"\n",
    "        ),\n",
    "        \"livelier\": (\n",
    "            \"Look for crowds, people walking or biking, street vendors, bright signage, open businesses, and dynamic movement. \"\n",
    "            \"Quiet, empty, or static scenes are less lively.\"\n",
    "        ),\n",
    "        \"more depressing\": (\n",
    "            \"Indicators include dark lighting, gray tones, boarded-up buildings, graffiti, trash, empty streets, and general neglect. \"\n",
    "            \"Fewer people and lack of activity can amplify the depressing feeling.\"\n",
    "        ),\n",
    "        \"safer\": (\n",
    "            \"Well-lit areas, visible pedestrians, clear pathways, greenery, maintained infrastructure, and surveillance signs suggest safety. \"\n",
    "            \"Broken lights, isolated alleys, graffiti, and damaged property indicate unsafety.\"\n",
    "        ),\n",
    "        \"more boring\": (\n",
    "            \"Uniform design, lack of people or variation, blank walls, closed businesses, and monotone architecture signal boredom. \"\n",
    "            \"Diversity in design and visible life make a place less boring.\"\n",
    "        )\n",
    "    }\n",
    "\n",
    "    return rules.get(study_question.lower(), \"No rule found for this category.\")\n",
    "\n",
    "for _, row in metadata_df.iterrows():\n",
    "    merged_index = row[\"merged_index\"]\n",
    "    study_question = row[\"study_question\"]\n",
    "    left_place = row[\"place_name_left\"]\n",
    "    right_place = row[\"place_name_right\"]\n",
    "    #print(left_place,right_place)\n",
    "    ground_truth = str(row[\"choice\"]).strip().lower()\n",
    "    image_path = os.path.join(image_dir, f\"merged_{merged_index:03d}.jpg\")\n",
    "\n",
    "    # Encode image to base64\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    img_byte_arr = io.BytesIO()\n",
    "    image.save(img_byte_arr, format=\"JPEG\")\n",
    "    base64_image = base64.b64encode(img_byte_arr.getvalue()).decode(\"utf-8\")\n",
    "\n",
    "    # Compose updated prompt\n",
    "    visual_perspective = get_visual_perspective(study_question) \n",
    "    #print(visual_perspective)\n",
    "    prompt_text = f\"\"\"\n",
    "        Compare two street view images taken in different cities — the left image is from {left_place}, and the right image is from {right_place}.\n",
    "        Your task is to decide which image better reflects the following quality:\n",
    "\n",
    "        \"{study_question}\"\n",
    "\n",
    "        Use the following rules when making your decision:\n",
    "\n",
    "        \"{visual_perspective}\"\n",
    "        \n",
    "        Then, apply the rules to the images and respond with a one-word judgment: left, right, or equal. Then explain your reasoning.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "    # Call LLM\n",
    "    response = client.chat.completions.create(\n",
    "        model= model_name,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": prompt_text.strip()},\n",
    "                    {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\"}},\n",
    "                ],\n",
    "            },\n",
    "        ],\n",
    "        response_format=character_schema,\n",
    "    )\n",
    "    full_response = json.loads(response.choices[0].message.content)\n",
    "\n",
    "    # Split judgment and reasoning\n",
    "    # Extract from dict\n",
    "    model_judgement = full_response.get(\"Judgment\", \"\").strip().lower()\n",
    "    model_reason = full_response.get(\"Reasons\", \"\").strip()\n",
    "\n",
    "    results.append({\n",
    "        \"merged_index\": merged_index,\n",
    "        \"left\": row[\"left\"],\n",
    "        \"right\": row[\"right\"],\n",
    "        \"study_question\": study_question,\n",
    "        \"ground_truth\": ground_truth,\n",
    "        \"model_judgement\": model_judgement,\n",
    "        \"model_reason\": model_reason,\n",
    "        \"validation\": int(model_judgement == ground_truth)\n",
    "    })\n",
    "\n",
    "# Save results\n",
    "df_result = pd.DataFrame(results)\n",
    "df_result.to_csv(f\"{output_dir}/llm_predictions_{model_name}_{prompt_method}.csv\", index=False)\n",
    "\n",
    "# Print accuracy\n",
    "# Accuracy including all responses\n",
    "accuracy_all = df_result[\"validation\"].mean()\n",
    "\n",
    "# Accuracy excluding any 'equal' in ground truth or model judgement\n",
    "filtered_df = df_result[\n",
    "    (df_result[\"ground_truth\"] != \"equal\") & \n",
    "    (df_result[\"model_judgement\"] != \"equal\")\n",
    "]\n",
    "accuracy_excl_equal = filtered_df[\"validation\"].mean()\n",
    "\n",
    "# Print both\n",
    "print(f\"✅ Accuracy (all): {accuracy_all:.2%}\")\n",
    "print(f\"✅ Accuracy (excluding 'equal'): {accuracy_excl_equal:.2%}\")\n",
    "print(\"✅ Output saved to llm_predictions.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831e8dc8",
   "metadata": {},
   "source": [
    "### RBIL_COT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d055c54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Accuracy (all): 56.00%\n",
      "✅ Accuracy (excluding 'equal'): 65.12%\n",
      "✅ Output saved to llm_predictions.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import base64\n",
    "import io\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# Load metadata\n",
    "prompt_method= \"RBIL_COT-GEO\"\n",
    "metadata_df = pd.read_csv(\"merged_metadata.csv\")\n",
    "image_dir = \"merged_images\"\n",
    "results = []\n",
    "\n",
    "def get_visual_perspective(study_question):\n",
    "    rules = {\n",
    "        \"wealthier\": (\n",
    "            \"Look for expensive cars, clean sidewalks, modern buildings, well-maintained facades, greenery, and overall tidiness. \"\n",
    "            \"Signs of poverty such as trash, broken sidewalks, and older buildings detract from the feeling of wealth.\"\n",
    "        ),\n",
    "        \"more beautiful\": (\n",
    "            \"Aesthetics matter: symmetry, architectural design, vibrant colors, trees, flowers, sunlight, and open space. \"\n",
    "            \"Avoid cluttered, gray, damaged, or visually unappealing features.\"\n",
    "        ),\n",
    "        \"livelier\": (\n",
    "            \"Look for crowds, people walking or biking, street vendors, bright signage, open businesses, and dynamic movement. \"\n",
    "            \"Quiet, empty, or static scenes are less lively.\"\n",
    "        ),\n",
    "        \"more depressing\": (\n",
    "            \"Indicators include dark lighting, gray tones, boarded-up buildings, graffiti, trash, empty streets, and general neglect. \"\n",
    "            \"Fewer people and lack of activity can amplify the depressing feeling.\"\n",
    "        ),\n",
    "        \"safer\": (\n",
    "            \"Well-lit areas, visible pedestrians, clear pathways, greenery, maintained infrastructure, and surveillance signs suggest safety. \"\n",
    "            \"Broken lights, isolated alleys, graffiti, and damaged property indicate unsafety.\"\n",
    "        ),\n",
    "        \"more boring\": (\n",
    "            \"Uniform design, lack of people or variation, blank walls, closed businesses, and monotone architecture signal boredom. \"\n",
    "            \"Diversity in design and visible life make a place less boring.\"\n",
    "        )\n",
    "    }\n",
    "\n",
    "    return rules.get(study_question.lower(), \"No rule found for this category.\")\n",
    "\n",
    "for _, row in metadata_df.iterrows():\n",
    "    merged_index = row[\"merged_index\"]\n",
    "    study_question = row[\"study_question\"]\n",
    "    left_place = row[\"place_name_left\"]\n",
    "    right_place = row[\"place_name_right\"]\n",
    "    ground_truth = str(row[\"choice\"]).strip().lower()\n",
    "    image_path = os.path.join(image_dir, f\"merged_{merged_index:03d}.jpg\")\n",
    "\n",
    "    # Encode image to base64\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    img_byte_arr = io.BytesIO()\n",
    "    image.save(img_byte_arr, format=\"JPEG\")\n",
    "    base64_image = base64.b64encode(img_byte_arr.getvalue()).decode(\"utf-8\")\n",
    "\n",
    "    # Compose updated prompt\n",
    "    visual_perspective = get_visual_perspective(study_question) \n",
    "    #print(visual_perspective)\n",
    "    prompt_text = f\"\"\"\n",
    "        Let’s think step by step.\n",
    "        Compare two street view images taken in different cities — the left image is from {left_place}, and the right image is from {right_place}.\n",
    "        Based on your overall impression, determine which one better reflects the following quality:\n",
    "\n",
    "        \"{study_question}\"\n",
    "\n",
    "         What features do you see in the left image? What features in the left image contribute to or detract from that quality?\n",
    "\n",
    "        What features do you see in the right image? What features in the right image contribute to or detract from that quality?\n",
    "\n",
    "        Based on the following rules, which image has features that better reflect the quality?\n",
    "\n",
    "        \"{visual_perspective}\"\n",
    "        \n",
    "        Then, apply the rules to the images and respond with a one-word judgment: left, right, or equal. Then explain your reasoning.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "    # Call LLM\n",
    "    response = client.chat.completions.create(\n",
    "        model= model_name,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": prompt_text.strip()},\n",
    "                    {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\"}},\n",
    "                ],\n",
    "            },\n",
    "        ],\n",
    "        response_format=character_schema,\n",
    "    )\n",
    "    full_response = json.loads(response.choices[0].message.content)\n",
    "\n",
    "    # Split judgment and reasoning\n",
    "    # Extract from dict\n",
    "    model_judgement = full_response.get(\"Judgment\", \"\").strip().lower()\n",
    "    model_reason = full_response.get(\"Reasons\", \"\").strip()\n",
    "\n",
    "    results.append({\n",
    "        \"merged_index\": merged_index,\n",
    "        \"left\": row[\"left\"],\n",
    "        \"right\": row[\"right\"],\n",
    "        \"study_question\": study_question,\n",
    "        \"ground_truth\": ground_truth,\n",
    "        \"model_judgement\": model_judgement,\n",
    "        \"model_reason\": model_reason,\n",
    "        \"validation\": int(model_judgement == ground_truth)\n",
    "    })\n",
    "\n",
    "# Save results\n",
    "df_result = pd.DataFrame(results)\n",
    "df_result.to_csv(f\"{output_dir}/llm_predictions_{model_name}_{prompt_method}.csv\", index=False)\n",
    "\n",
    "# Print accuracy\n",
    "# Accuracy including all responses\n",
    "accuracy_all = df_result[\"validation\"].mean()\n",
    "\n",
    "# Accuracy excluding any 'equal' in ground truth or model judgement\n",
    "filtered_df = df_result[\n",
    "    (df_result[\"ground_truth\"] != \"equal\") & \n",
    "    (df_result[\"model_judgement\"] != \"equal\")\n",
    "]\n",
    "accuracy_excl_equal = filtered_df[\"validation\"].mean()\n",
    "\n",
    "# Print both\n",
    "print(f\"✅ Accuracy (all): {accuracy_all:.2%}\")\n",
    "print(f\"✅ Accuracy (excluding 'equal'): {accuracy_excl_equal:.2%}\")\n",
    "print(\"✅ Output saved to llm_predictions.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GraphRAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
