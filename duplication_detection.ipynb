{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9c38c717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§  Unique study_question values:\n",
      "- wealthier\n",
      "- more beautiful\n",
      "- livelier\n",
      "- more depressing\n",
      "- safer\n",
      "- more boring\n",
      "- nan\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load CSV\n",
    "df = pd.read_csv(\"votes_clean.csv\")\n",
    "# Get unique study_question values\n",
    "unique_questions = df[\"study_question\"].unique()\n",
    "\n",
    "# Print them\n",
    "print(\"ðŸ§  Unique study_question values:\")\n",
    "for q in unique_questions:\n",
    "    print(f\"- {q}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e2cd17a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Updated summary with 'equal' saved to 'repeated_pairs_vote_summary.csv'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zwang31\\AppData\\Local\\Temp\\ipykernel_13400\\212485569.py:34: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(summarize_group)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load CSV\n",
    "df = pd.read_csv(\"votes_clean.csv\")\n",
    "\n",
    "# Create unordered pair key\n",
    "df[\"pair_key\"] = df.apply(lambda row: frozenset([str(row[\"left\"]), str(row[\"right\"])]), axis=1)\n",
    "\n",
    "# Count occurrences\n",
    "pair_counts = df[\"pair_key\"].value_counts()\n",
    "repeating_pairs = pair_counts[pair_counts > 1]\n",
    "\n",
    "# Filter repeated rows\n",
    "duplicates_df = df[df[\"pair_key\"].isin(repeating_pairs.index)].copy()\n",
    "\n",
    "# Normalize left/right\n",
    "duplicates_df[[\"left_sorted\", \"right_sorted\"]] = duplicates_df.apply(\n",
    "    lambda row: pd.Series(sorted([str(row[\"left\"]), str(row[\"right\"])])), axis=1\n",
    ")\n",
    "\n",
    "# Group and summarize\n",
    "def summarize_group(group):\n",
    "    return pd.Series({\n",
    "        \"left\": (group[\"choice\"] == \"left\").sum(),\n",
    "        \"right\": (group[\"choice\"] == \"right\").sum(),\n",
    "        \"equal\": (group[\"choice\"] == \"equal\").sum(),\n",
    "        \"total\": len(group),\n",
    "        \"study_questions\": list(group[\"study_question\"].unique())\n",
    "    })\n",
    "\n",
    "summary_df = (\n",
    "    duplicates_df\n",
    "    .groupby([\"left_sorted\", \"right_sorted\"])\n",
    "    .apply(summarize_group)\n",
    "    .reset_index()\n",
    "    .sort_values(\"total\", ascending=False)\n",
    ")\n",
    "\n",
    "# Save to CSV\n",
    "summary_df.to_csv(\"repeated_pairs_vote_summary.csv\", index=False)\n",
    "print(\"âœ… Updated summary with 'equal' saved to 'repeated_pairs_vote_summary.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce857de1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Load CSV\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvotes_clean.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Remove rows where choice is 'equal'\u001b[39;00m\n\u001b[0;32m      7\u001b[0m df \u001b[38;5;241m=\u001b[39m df[df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchoice\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mlower()\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mequal\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mcopy()\n",
      "File \u001b[1;32mc:\\Users\\zwang31\\.conda\\envs\\GraphRAG\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\zwang31\\.conda\\envs\\GraphRAG\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:626\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[0;32m    625\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[1;32m--> 626\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\zwang31\\.conda\\envs\\GraphRAG\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1923\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1916\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[0;32m   1917\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1918\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[0;32m   1919\u001b[0m     (\n\u001b[0;32m   1920\u001b[0m         index,\n\u001b[0;32m   1921\u001b[0m         columns,\n\u001b[0;32m   1922\u001b[0m         col_dict,\n\u001b[1;32m-> 1923\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[0;32m   1924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[0;32m   1925\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1926\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1927\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\zwang31\\.conda\\envs\\GraphRAG\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[1;32m--> 234\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    235\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[0;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[1;32mparsers.pyx:838\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:905\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:874\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:891\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:2061\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mParserError\u001b[0m: Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load CSV\n",
    "df = pd.read_csv(\"votes_clean.csv\")\n",
    "\n",
    "# Remove rows where choice is 'equal'\n",
    "df = df[df[\"choice\"].str.lower().str.strip() != \"equal\"].copy()\n",
    "\n",
    "# Normalize left/right (unordered)\n",
    "df[[\"left_sorted\", \"right_sorted\"]] = df.apply(\n",
    "    lambda row: pd.Series(sorted([str(row[\"left\"]), str(row[\"right\"])])), axis=1\n",
    ")\n",
    "\n",
    "# Group by (left, right, study_question)\n",
    "grouped = (\n",
    "    df.groupby([\"left_sorted\", \"right_sorted\", \"study_question\"])\n",
    "    .agg(\n",
    "        left_num=(\"choice\", lambda x: (x == \"left\").sum()),\n",
    "        right_num=(\"choice\", lambda x: (x == \"right\").sum()),\n",
    "        total=(\"choice\", \"count\")\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Keep only duplicated (total > 1)\n",
    "duplicates_only = grouped[grouped[\"total\"] > 1].copy()\n",
    "\n",
    "# Rename columns for output\n",
    "duplicates_only.rename(columns={\n",
    "    \"left_sorted\": \"left_image_ID\",\n",
    "    \"right_sorted\": \"right_image_ID\"\n",
    "}, inplace=True)\n",
    "\n",
    "# Sort by total\n",
    "duplicates_only = duplicates_only.sort_values(\"total\", ascending=False)\n",
    "\n",
    "# Save\n",
    "duplicates_only.to_csv(\"repeated_pair_summary_filtered.csv\", index=False)\n",
    "print(\"âœ… Saved duplicates only to 'repeated_pair_summary_filtered.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d5c54247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Unnamed: 0             place_id_left            place_id_right  \\\n",
      "322589      322590  50e6fcabd7c3df413b000aa6  50e74a36d7c3df413b001488   \n",
      "323306      323307  50e6fcabd7c3df413b000aa6  50e74a36d7c3df413b001488   \n",
      "323401      323402  50e6fcabd7c3df413b000aa6  50e74a36d7c3df413b001488   \n",
      "323403      323404  50e6fcabd7c3df413b000aa6  50e74a36d7c3df413b001488   \n",
      "323437      323438  50e6fcabd7c3df413b000aa6  50e74a36d7c3df413b001488   \n",
      "...            ...                       ...                       ...   \n",
      "355356      355357  50e6fcabd7c3df413b000aa6  50e74a36d7c3df413b001488   \n",
      "355480      355481  50e6fcabd7c3df413b000aa6  50e74a36d7c3df413b001488   \n",
      "356011      356012  50e6fcabd7c3df413b000aa6  50e74a36d7c3df413b001488   \n",
      "356344      356345  50e6fcabd7c3df413b000aa6  50e74a36d7c3df413b001488   \n",
      "356744      356745  50e6fcabd7c3df413b000aa6  50e74a36d7c3df413b001488   \n",
      "\n",
      "                        study_id                      left  \\\n",
      "322589  50f62c41a84ea7c5fdd2e454  513e5dc3fdc9f0358700aeab   \n",
      "323306  50f62c41a84ea7c5fdd2e454  513e5dc3fdc9f0358700aeab   \n",
      "323401  50f62c41a84ea7c5fdd2e454  513e5dc3fdc9f0358700aeab   \n",
      "323403  50f62c41a84ea7c5fdd2e454  513e5dc3fdc9f0358700aeab   \n",
      "323437  50f62c41a84ea7c5fdd2e454  513e5dc3fdc9f0358700aeab   \n",
      "...                          ...                       ...   \n",
      "355356  50f62c41a84ea7c5fdd2e454  513e5dc3fdc9f0358700aeab   \n",
      "355480  50f62c41a84ea7c5fdd2e454  513e5dc3fdc9f0358700aeab   \n",
      "356011  50f62c41a84ea7c5fdd2e454  513e5dc3fdc9f0358700aeab   \n",
      "356344  50f62c41a84ea7c5fdd2e454  513e5dc3fdc9f0358700aeab   \n",
      "356744  50f62c41a84ea7c5fdd2e454  513e5dc3fdc9f0358700aeab   \n",
      "\n",
      "                           right                    voter_uniqueid  \\\n",
      "322589  5140d960fdc9f04926003bb4  eb5b2e112cb848899d4c73ee3c359417   \n",
      "323306  5140d960fdc9f04926003bb4  eb5b2e112cb848899d4c73ee3c359417   \n",
      "323401  5140d960fdc9f04926003bb4  eb5b2e112cb848899d4c73ee3c359417   \n",
      "323403  5140d960fdc9f04926003bb4  eb5b2e112cb848899d4c73ee3c359417   \n",
      "323437  5140d960fdc9f04926003bb4  eb5b2e112cb848899d4c73ee3c359417   \n",
      "...                          ...                               ...   \n",
      "355356  5140d960fdc9f04926003bb4  eb5b2e112cb848899d4c73ee3c359417   \n",
      "355480  5140d960fdc9f04926003bb4  eb5b2e112cb848899d4c73ee3c359417   \n",
      "356011  5140d960fdc9f04926003bb4  eb5b2e112cb848899d4c73ee3c359417   \n",
      "356344  5140d960fdc9f04926003bb4  eb5b2e112cb848899d4c73ee3c359417   \n",
      "356744  5140d960fdc9f04926003bb4  eb5b2e112cb848899d4c73ee3c359417   \n",
      "\n",
      "                                             choice study_question  \\\n",
      "322589                                         left       livelier   \n",
      "323306                                        right       livelier   \n",
      "323401                         $(nslookup ErfMv0bx)       livelier   \n",
      "323403                                          NaN       livelier   \n",
      "323437   -1' OR 2+320-320-1=0+0+0+1 or 'H87wbQAd'='       livelier   \n",
      "...                                             ...            ...   \n",
      "355356                                         left       livelier   \n",
      "355480                                         left       livelier   \n",
      "356011                          ${10000266+9999972}       livelier   \n",
      "356344                                         left       livelier   \n",
      "356744  ${@print(md5(acunetix_wvs_security_test))}\\       livelier   \n",
      "\n",
      "       place_name_right place_name_left         day      time  long_right  \\\n",
      "322589           Dublin           Paris  2018-08-27  02:04:28   53.329323   \n",
      "323306           Dublin           Paris  2018-08-27  02:05:15   53.329323   \n",
      "323401           Dublin           Paris  2018-08-27  02:04:21   53.329323   \n",
      "323403           Dublin           Paris  2018-08-27  02:04:20   53.329323   \n",
      "323437           Dublin           Paris  2018-08-27  02:04:23   53.329323   \n",
      "...                 ...             ...         ...       ...         ...   \n",
      "355356           Dublin           Paris  2018-08-27  02:00:49   53.329323   \n",
      "355480           Dublin           Paris  2018-08-27  02:05:01   53.329323   \n",
      "356011           Dublin           Paris  2018-08-27  02:04:21   53.329323   \n",
      "356344           Dublin           Paris  2018-08-27  02:04:43   53.329323   \n",
      "356744           Dublin           Paris  2018-08-27  02:04:28   53.329323   \n",
      "\n",
      "        lat_right  long_left  lat_left  \n",
      "322589  -6.231007  48.878382  2.403116  \n",
      "323306  -6.231007  48.878382  2.403116  \n",
      "323401  -6.231007  48.878382  2.403116  \n",
      "323403  -6.231007  48.878382  2.403116  \n",
      "323437  -6.231007  48.878382  2.403116  \n",
      "...           ...        ...       ...  \n",
      "355356  -6.231007  48.878382  2.403116  \n",
      "355480  -6.231007  48.878382  2.403116  \n",
      "356011  -6.231007  48.878382  2.403116  \n",
      "356344  -6.231007  48.878382  2.403116  \n",
      "356744  -6.231007  48.878382  2.403116  \n",
      "\n",
      "[107 rows x 17 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load CSV\n",
    "df = pd.read_csv(\"votes_clean.csv\")\n",
    "\n",
    "# Normalize for unordered pair comparison\n",
    "target_ids = {\"513e5dc3fdc9f0358700aeab\", \"5140d960fdc9f04926003bb4\"}\n",
    "target_question = \"livelier\"\n",
    "\n",
    "filtered_df = df[\n",
    "    (df[\"study_question\"].str.strip().str.lower() == target_question.lower()) &\n",
    "    (df.apply(lambda row: {str(row[\"left\"]), str(row[\"right\"])} == target_ids, axis=1))\n",
    "]\n",
    "\n",
    "\n",
    "# Show results\n",
    "print(filtered_df)\n",
    "\n",
    "# Optional: save to file\n",
    "filtered_df.to_csv(\"filtered_livelier_pair.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cdf09e75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved to 'repeated_pair_summary_filtered_no_ties.csv' with updated totals\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load file\n",
    "df = pd.read_csv(\"repeated_pair_summary_filtered.csv\")\n",
    "\n",
    "# Remove ties\n",
    "df = df[df[\"left_num\"] != df[\"right_num\"]].copy()\n",
    "\n",
    "# Recalculate total\n",
    "df[\"total\"] = df[\"left_num\"] + df[\"right_num\"]\n",
    "\n",
    "# Save to new file\n",
    "df.to_csv(\"repeated_pair_summary_filtered_no_ties.csv\", index=False)\n",
    "print(\"âœ… Saved to 'repeated_pair_summary_filtered_no_ties.csv' with updated totals\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b6a7840b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved with left_num/right_num to 'representative_votes_clean_format.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load data\n",
    "summary_df = pd.read_csv(\"repeated_pair_summary_filtered_no_ties.csv\")\n",
    "votes_df = pd.read_csv(\"votes_clean.csv\")\n",
    "\n",
    "# Normalize and prep votes_clean\n",
    "votes_df[\"left\"] = votes_df[\"left\"].astype(str)\n",
    "votes_df[\"right\"] = votes_df[\"right\"].astype(str)\n",
    "votes_df[\"choice\"] = votes_df[\"choice\"].str.strip().str.lower()\n",
    "votes_df[\"study_question\"] = votes_df[\"study_question\"].str.strip().str.lower()\n",
    "votes_df[\"pair_key\"] = votes_df.apply(lambda r: \",\".join(sorted([r[\"left\"], r[\"right\"]])), axis=1)\n",
    "\n",
    "# Normalize summary_df\n",
    "summary_df[\"left_image_ID\"] = summary_df[\"left_image_ID\"].astype(str)\n",
    "summary_df[\"right_image_ID\"] = summary_df[\"right_image_ID\"].astype(str)\n",
    "summary_df[\"study_question\"] = summary_df[\"study_question\"].str.strip().str.lower()\n",
    "summary_df[\"pair_key\"] = summary_df.apply(\n",
    "    lambda r: \",\".join(sorted([r[\"left_image_ID\"], r[\"right_image_ID\"]])), axis=1\n",
    ")\n",
    "summary_df[\"choice\"] = summary_df.apply(\n",
    "    lambda r: \"left\" if r[\"left_num\"] > r[\"right_num\"] else \"right\", axis=1\n",
    ")\n",
    "\n",
    "# Merge and keep left_num, right_num\n",
    "merged = pd.merge(\n",
    "    summary_df[[\"pair_key\", \"study_question\", \"choice\", \"left_num\", \"right_num\"]],\n",
    "    votes_df,\n",
    "    how=\"left\",\n",
    "    on=[\"pair_key\", \"study_question\", \"choice\"]\n",
    ")\n",
    "\n",
    "# Drop duplicates by pair_key + study_question\n",
    "merged = merged.drop_duplicates(subset=[\"pair_key\", \"study_question\"])\n",
    "\n",
    "# Save all original votes_clean columns + left_num and right_num\n",
    "final_cols = list(votes_df.columns) + [\"left_num\", \"right_num\"]\n",
    "final_df = merged[final_cols]\n",
    "\n",
    "final_df.to_csv(\"representative_votes_clean_format.csv\", index=False)\n",
    "print(\"âœ… Saved with left_num/right_num to 'representative_votes_clean_format.csv'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GraphRAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
